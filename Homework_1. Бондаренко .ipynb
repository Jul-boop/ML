{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия и векторное дифференцирование (10 баллов).\n",
    "\n",
    "* Максимальное количество баллов за задания в ноутбуке - 11, но больше 10 оценка не ставится, поэтому для получения максимальной оценки можно сделать не все задания.\n",
    "\n",
    "* Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмите остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 вариант "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000, random_state=686)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2900575709000693e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.744985494322756e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.76783042e-08, -4.53842273e-08, -2.36903536e-08, -1.32251825e-09,\n",
       "        4.75085869e-08,  3.57233559e-08,  4.57344264e-08, -3.14619725e-08,\n",
       "        2.34091496e-09,  8.01573064e-09,  3.81918705e+01, -1.96456194e-08,\n",
       "       -1.70073003e-08, -2.76973133e-08,  8.41441400e-08,  5.80866527e-08,\n",
       "       -2.70180968e-08,  1.91161239e-08,  1.37219170e-08, -2.68484909e-08,\n",
       "       -1.57405107e-08,  1.20871593e-08, -3.95256327e-08, -1.70886247e-08,\n",
       "       -2.86745159e-08,  1.43282656e-08,  1.34941028e-08, -4.72163875e-08,\n",
       "        2.61457594e-08, -1.73034771e-08,  3.51196502e-08,  3.66861472e-08,\n",
       "       -5.23528532e-08,  1.48429211e-08,  1.53621545e-08,  4.25533222e-08,\n",
       "       -2.63555936e-08, -5.40402157e-08,  1.91010779e-08,  4.86898160e-08,\n",
       "        2.07871111e+01, -3.68272019e-08,  6.29339627e-08, -1.86099695e-08,\n",
       "       -1.26148609e-08, -4.77004073e-08, -4.68199665e-08, -3.63329883e-09,\n",
       "       -6.75544743e-08,  2.22005612e-08,  7.62186312e-08, -4.25714429e-09,\n",
       "       -1.26620626e-08,  2.59578487e-08,  4.82523598e-08, -1.25065845e-08,\n",
       "        5.84032775e+01, -4.75045886e-08,  6.09955595e-08, -1.43524309e-08,\n",
       "        3.27008187e-08,  9.70322222e+01,  5.64799375e-08,  4.75266192e-08,\n",
       "        5.14502108e-08, -4.27107627e-08,  5.16397716e-08,  7.55193951e-09,\n",
       "       -2.61102116e-08,  6.24774328e-08,  1.20075939e-08, -5.33536412e-08,\n",
       "        4.16800682e+01, -2.29748102e-08,  1.39991187e-08,  1.70234545e+01,\n",
       "       -4.57251978e-08,  1.08313160e-08, -2.19415595e-08, -3.04659449e-08,\n",
       "       -4.58632347e-08,  1.83586110e-08,  9.06260637e+01, -7.51950015e-08,\n",
       "       -1.81771126e-08,  3.48512884e-08,  2.12869946e-08,  7.00386397e-09,\n",
       "       -6.12763031e-08, -6.56596127e-09,  8.94468316e+01,  2.38488925e+01,\n",
       "       -5.00994031e-08,  5.94536733e+01, -2.46179425e-09, -5.67262119e-08,\n",
       "       -1.35595379e-09, -5.17815380e-08,  3.14913301e-09,  1.76463105e-08])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001, random_state=686).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объясните, чем вызвано различие двух полученных значений метрики?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Значение метрики при обучение линейной регрессии, полученное с использованием аналитической формулы: $1.2900575709000693e-25$\n",
    "\n",
    " Значение метрики при обучение линейной регрессии методом градиентного спуска : $3.744985494322756e-12$\n",
    "\n",
    "$$1.2900575709000693e-25 < 3.744985494322756e-12$$\n",
    "\n",
    "  Метод стохастического градиентного спуска на каждой итерации выбирает один случайный объект и сдвигается в сторону антиградиента функции потерь по этому объекту - то есть в направлении уменьшении ошибки по этому объекту. При грамотно подобранной величине стохастического градиентного шага через некоторое число шагов мы попадем в некоторую малую окрестность точки минимума. Однако гипермараметры могут быть подобраны недостаточно хорошо, что не позволит гарантированно достичь самой точки минимума, полученной аналитическим решением. В свою очередь аналитическое решение однозначно находит эту точку минимума. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132b3bea1a7846f2acef4a5fb467d7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Лучшая alpha: 1e-16\n",
      "Лучшee Penalty: l1\n",
      "Лучшeе MSE: 7.623786944622722e-25\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "alphas = 1 / 10 ** (np.arange(19))\n",
    "penalties = ['l2', 'l1', 'elasticnet']\n",
    "\n",
    "best_alpha = 0\n",
    "best_penalty = 'l2'\n",
    "best_mse = 100\n",
    "\n",
    "for alpha in tqdm(alphas):\n",
    "    for penalty in penalties:\n",
    "        reg = SGDRegressor(alpha=alpha, penalty=penalty, random_state=686).fit(X, y)\n",
    "        current_mse = mean_squared_error(y, reg.predict(X))\n",
    "        if current_mse < best_mse:\n",
    "            best_alpha = alpha\n",
    "            best_penalty = penalty\n",
    "            best_mse = current_mse\n",
    "\n",
    "print('Лучшая alpha:', best_alpha)\n",
    "print('Лучшee Penalty:', best_penalty)\n",
    "print('Лучшeе MSE:',best_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Альтернативный способ поиска через GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1e-15, 'penalty': 'l1'}\n",
      "MSE: 8.239927115683944e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "reg = SGDRegressor(random_state=686)\n",
    "num_splits = 5\n",
    "alphas = 1 / 10 ** (np.arange(19))\n",
    "penalties = ['l2', 'l1', 'elasticnet']\n",
    "\n",
    "best_alpha = 0\n",
    "best_penalty = 'l2'\n",
    "best_mse = 100\n",
    "\n",
    "\n",
    "\n",
    "params = {'alpha':alphas,\n",
    "         'penalty':penalties,\n",
    "          }\n",
    "    \n",
    "    \n",
    "# print(params)\n",
    "cv = GridSearchCV(reg,\n",
    "                  params,\n",
    "                  scoring='neg_mean_squared_error',\n",
    "                  cv=num_splits, \n",
    "                  n_jobs=-1\n",
    "                 )\n",
    "cv.fit(X, y)\n",
    "\n",
    "print(cv.best_params_)\n",
    "\n",
    "reg = SGDRegressor(**cv.best_params_, random_state=686)\n",
    "reg.fit(X, y)\n",
    "print('MSE:', mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, полученное ранее значение (аналитически) MSE было: $1.2900575709000693e^{−25}$\n",
    "\n",
    "Полученное лучшее значение MSE при подборе гиперпараметров (согласно первому методу поиска): $7.623786944622722e^{-25}$ \n",
    "\n",
    "при альтернативном варианте: $8.239927115683944e^{-25}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом градиентного спуска. Для этого используйте шаблонный класс.**\n",
    "\n",
    "Критерий останова: либо **норма разности весов на текущей и предыдущей итерациях меньше определенного значения** (первый и **третий варианты**), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.0001, l_ratio=0.001, tol=0.001, max_iter=1000):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        alpha - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "             \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        self.w = np.random.randn(X.shape[1]+1) \n",
    "        X = add_constant(X)\n",
    "\n",
    "        for i in tqdm(range(self.max_iter)):\n",
    "            grad = -2 / X.shape[0] * X.T @ (y - X @ self.w) \n",
    "            self.w -= self.alpha * grad\n",
    "            \n",
    "            if np.linalg.norm(self.alpha * grad) < self.tol:\n",
    "                break\n",
    "                \n",
    "        # self.w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "   \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        X = add_constant(X)\n",
    "        return X @ self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ad0c75183a45439dc71787acb470ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression(alpha=0.04, l_ratio=0.0001, tol=1e-5, max_iter=1000)\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_linear = mean_squared_error(y, my_reg.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимизированная MSE методом градиентного спуска: 1.4833032292516285e-08\n"
     ]
    }
   ],
   "source": [
    "print ('Оптимизированная MSE методом градиентного спуска:', mean_squared_error(y, my_reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавьте l1 (первый и второй варианты) или **l2** (**третий** и четвертый варианты) регуляризацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.0001, l_ratio=0.001, tol=0.001, max_iter=1000):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        alpha - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "             \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        self.w = np.random.randn(X.shape[1]+1) \n",
    "        X = add_constant(X)\n",
    "\n",
    "        for i in tqdm(range(self.max_iter)):\n",
    "#             grad = -2 / X.shape[0] * X.T @ (y - X @ self.w) \n",
    "            grad = (-2 / X.shape[0]) * X.T @ (y - X @ self.w) + 2 * self.l_ratio * self.w\n",
    "            self.w -= self.alpha * grad\n",
    "            \n",
    "            if np.linalg.norm(self.alpha * grad) < self.tol:\n",
    "                break\n",
    "                \n",
    "        # self.w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "   \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        X = add_constant(X)\n",
    "        return X @ self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5a530342b54bd685733fbe8470334e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression(alpha=0.04, l_ratio=0.0001, tol=1e-5, max_iter=10000)\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003766660421207986"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y, my_reg.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE без регуляризации L2: 1.4833032292516285e-08\n",
      "MSE c регуляризацией  L2: 0.0003766660421207986\n"
     ]
    }
   ],
   "source": [
    "print ('MSE без регуляризации L2:',mse_linear)\n",
    "print ('MSE c регуляризацией  L2:', mean_squared_error(y, my_reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучите линейную регрессию из коробки**\n",
    "\n",
    "* с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и второй вариант) или с **l2-регуляризацией (from sklearn.linear_model import Ridge, третий** и четвертый вариант)\n",
    "* со значением параметра регуляризации **0.1** - для первого и **третьего варианта**, 0.01 - для второго и четвертого варианта. \n",
    "\n",
    "**Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае линеной регрессии, когда модель переобучается - те становится слишком специфичной и теряет обобщающую способность, используют регуляризацией для борьбы с переобучением соотвественно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация Ridge или L2-регуляризация:\n",
    "\n",
    "$Q_{ridge}(w) = Q(w) + \\alpha \\Sigma_{j=0}^{k}w_{k}^{2}$\n",
    "\n",
    "\n",
    "Несмотря на то, что оба вида регуляризации ведут к занижению значений весов, в случае Ridge регрессии веса могут быть сколько угодно близки к 0, но никогда не равны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7388791425941274e-06"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)\n",
    "mse_sklearn = mean_squared_error(y, ridge.predict(X))\n",
    "mse_sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8651d85d8dd43c198dd5b39ffb41a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "307.8287498768968"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_reg = LinearRegression(alpha=0.01, max_iter=10000, tol=1e-6, l_ratio=0.1)\n",
    "my_reg.fit(X, y)\n",
    "mean_squared_error(y, my_reg.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Была попытка посмотреть с меньшим шагом и с большим количеством итераций соответсвенно (прогонял полчаса)-результат не изменился"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MSE Ridge: 3.7388791425941274e-06\n",
      " MSE Linear regression: 307.8287498768968\n"
     ]
    }
   ],
   "source": [
    "print (' MSE Ridge:',mean_squared_error(y, ridge.predict(X)) )\n",
    "print (' MSE Linear regression:',mean_squared_error(y, my_reg.predict(X)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По линейной регресии - чем больше параметр регуляризации - тем более сильно искажен градиент (посмотрим формулу): $$grad = (-2 / X.shape[0]) * X.T @ (y - X @ self.w) + 2 * self.lratio * self.w$$\n",
    "\n",
    "Это может привести к тому, что спуск не достигнет точки минимума - может и вовсе уйти в сторону в связи с искажением градиента. \n",
    "\n",
    "Поэтому получаем качество метрики соотвественно хуже. Необходимо взять параметр регуляризации менее 0.1, что позволит снизить искажение градиента и достичь ошибку меньше соотвественно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3b70194aa84cda805c22ce579d4685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.664947022503247"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_reg = LinearRegression(alpha=0.01, max_iter=10000, tol=1e-5, l_ratio=0.01)\n",
    "my_reg.fit(X, y)\n",
    "mean_squared_error(y, my_reg.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 6* (1 балл).***\n",
    "Пусть $P, Q \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_Q tr(PQ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*загружен в anytask*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 7* (1 балл).***\n",
    "Пусть $x, y \\in \\mathbb{R}^{n}, M \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_M x^T M y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *загружен в anytask*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения заданий 6 и 7 можно написать на листочке и отправить в anytask вместе с заполненным ноутбуком."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
